<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/static/css/tango.css">
        <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/favicon.ico" type="image/x-icon">
        <title>Spark RDD及编程接口 - 老顽童的wiki</title>
        <meta name="keywords" content="wiki, python, machine learning"/>
        <meta name="description" content="wiki-zhb is a wiki for zhbzz2007."/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/">Home</a>&nbsp;»&nbsp;<a href="/#Spark">Spark</a>&nbsp;»&nbsp;Spark RDD及编程接口</div>
</div>
<div class="clearfix"></div>
<div id="title">Spark RDD及编程接口</div>
  <div id="content">
  <h2 id="1spark-rdd">1.Spark RDD</h2>
<p>RDD(Resilient Distributed Datasets),弹性分布式数据集，即一个RDD代表一个被分区的只读数据集。一个RDD的生成只有两种途径，一是来自内存集合和外部存储系统，另一种是通过转换操作来自于其它RDD，比如map、filter、join等等。</p>
<p>RDD没必要随时被实例化，由于RDD的接口只支持粗粒度的操作（即一个操作会被应用在RDD的所有数据上），所有只要通过记录下这些作用在RDD之上的转换操作，来构建RDD的继承关系（lineage），就可以有效地进行容错处理，而不需要将实际的RDD数据进行记录拷贝。这对于RDD来说是一项非常强大的功能，也即在一个Spark程序中，我们所用到的每一个RDD，在丢失或者操作失败后都是可以重建的。</p>
<p>开发者还可以对RDD进行另外两个方面的控制操作：持久化和分区。开发者可以指明它们需要重用哪些RDD，然后选择一种存储策略（如in-memory storage）将它们保存起来。开发者还可以让RDD根据记录中的键值在集群的机器之间重新分区。</p>
<p>抽象的RDD采用如下五个接口来表示一个分区的、高效容错的而且能够持久化的分布式数据集。</p>
<table>
<thead>
<tr>
<th>RDD接口</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>partition</td>
<td>分区，一个RDD会有一个或者多个分区</td>
</tr>
<tr>
<td>preferredLocations(p)</td>
<td>对于分区p而言，返回数据本地化计算的节点</td>
</tr>
<tr>
<td>dependencies()</td>
<td>RDD的依赖关系</td>
</tr>
<tr>
<td>compute(p,context)</td>
<td>对于分区p而言，进行迭代计算</td>
</tr>
<tr>
<td>partitioner()</td>
<td>RDD的分区函数</td>
</tr>
</tbody>
</table>
<h3 id="11rddpartitions">1.1RDD分区(partitions)</h3>
<p>对于RDD的分区而言，用户可以自行指定多少分区，如果没有指定，将会使用默认值。可以利用RDD的成员变量partitions所返回的partition数组的大小来查询一个RDD被划分的分区数。</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="nx">sc.parallelize</span><span class="p">(</span><span class="mi">1</span> <span class="k">to</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="c1">//指定分区为2</span>
<span class="nx">rdd</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">0</span><span class="cp">]</span> at parallelize at <span class="nt">&lt;console&gt;</span>:27

scala&gt; rdd.partitions.size
res0: Int = 2
</pre></div>


<p>创建RDD的时候不指定分区，系统默认的数值就是这个程序所分配到的资源的CPU核的个数。</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="nx">sc.parallelize</span><span class="p">(</span><span class="mi">1</span> <span class="k">to</span> <span class="mi">100</span><span class="p">)</span><span class="c1">//不指定分区</span>
<span class="nx">rdd</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">2</span><span class="cp">]</span> at parallelize at <span class="nt">&lt;console&gt;</span>:27

scala&gt; rdd.partitions.size
res2: Int = 4
</pre></div>


<h3 id="12rddpreferred-locations">1.2RDD优先位置(preferred Locations)</h3>
<p>RDD优先位置于Spark中的调度有关，返回的是此RDD的每个partition所存储的位置，按照“移动数据不如移动计算”的理念，在Spark进行任务调度的时候，尽可能地将任务分配到数据块所存储的位置，若以从Hadoop中读取数据生成RDD为例，preferredLocations返回每一个数据块所在的机器名或者IP地址，如果每一块数据是多份存储的，那么就会返回多个机器地址。本例中以ml-100k数据为例，返回一个空的ListBuffer。</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="nx">sc.textFile</span><span class="p">(</span><span class="s2">&quot;/home/zhb/Desktop/work/SparkData/ml-100k&quot;</span><span class="p">)</span>
<span class="nx">rdd</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="kt">String</span><span class="cp">]</span> = /home/zhb/Desktop/work/SparkData/ml-100k MapPartitionsRDD<span class="cp">[</span><span class="mi">4</span><span class="cp">]</span> at textFile at <span class="nt">&lt;console&gt;</span>:27

scala&gt; val rdd = sc.textFile(&quot;/home/zhb/Desktop/work/SparkData/ml-100k/u.user&quot;)
rdd: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="kt">String</span><span class="cp">]</span> = /home/zhb/Desktop/work/SparkData/ml-100k/u.user MapPartitionsRDD<span class="cp">[</span><span class="mi">6</span><span class="cp">]</span> at textFile at <span class="nt">&lt;console&gt;</span>:27

scala&gt; val hadoopRDD = rdd.dependencies(0).rdd
hadoopRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="nx">_</span><span class="cp">]</span> = /home/zhb/Desktop/work/SparkData/ml-100k/u.user HadoopRDD<span class="cp">[</span><span class="mi">5</span><span class="cp">]</span> at textFile at <span class="nt">&lt;console&gt;</span>:27

scala&gt; hadoopRDD.partitions.size
res3: Int = 2

scala&gt; hadoopRDD.preferredLocations(hadoopRDD.partitions(0))
res4: Seq<span class="cp">[</span><span class="kt">String</span><span class="cp">]</span> = ListBuffer()
</pre></div>


<h3 id="13rdddependencies">1.3RDD依赖关系(dependencies)</h3>
<p>RDD是粗粒度的操作数据集，每个转换操作都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系，在Spark中存在两种类型的依赖，即窄依赖（Narrow Dependencies）和宽依赖（Wide Dependencies）。</p>
<p>窄依赖：每一个父RDD的分区最多只被一个子RDD的一个分区所使用。</p>
<p>宽依赖：多个子RDD的分区会依赖同一个父RDD的分区。</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="nx">sc.makeRDD</span><span class="p">(</span><span class="mi">1</span> <span class="k">to</span> <span class="mi">10</span><span class="p">)</span>
<span class="nx">rdd</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">7</span><span class="cp">]</span> at makeRDD at <span class="nt">&lt;console&gt;</span>:27

scala&gt; val mapRDD = rdd.map(x =&gt; (x,x))
mapRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = MapPartitionsRDD<span class="cp">[</span><span class="mi">8</span><span class="cp">]</span> at map at <span class="nt">&lt;console&gt;</span>:29

scala&gt; mapRDD.dependencies
res5: Seq<span class="cp">[</span><span class="nx">org.apache.spark.Dependency</span><span class="err">[</span><span class="nx">_</span><span class="cp">]</span>] = List(org.apache.spark.OneToOneDependency@5b79ff65)

scala&gt; val shuffleRDD = mapRDD.partitionBy(new org.apache.spark.HashPartitioner(3))
shuffleRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = ShuffledRDD<span class="cp">[</span><span class="mi">9</span><span class="cp">]</span> at partitionBy at <span class="nt">&lt;console&gt;</span>:31

scala&gt; shuffleRDD.dependencies
res6: Seq<span class="cp">[</span><span class="nx">org.apache.spark.Dependency</span><span class="err">[</span><span class="nx">_</span><span class="cp">]</span>] = List(org.apache.spark.ShuffleDependency@17d42714)
scala&gt; val rdd = sc.makeRDD(collect)
</pre></div>


<p>rdd: org.apache.spark.rdd.RDD[scala.collection.immutable.Range.Inclusive] = ParallelCollectionRDD[1] at makeRDD at <console>:29</p>
<h3 id="14rddcompute">1.4RDD分区计算(compute)</h3>
<p>Spark中每个RDD的计算都是以partition（分区）为单位的，而且RDD中的compute函数都是在迭代器进行复合，不需要保存每次计算的结果。</p>
<h3 id="15rddpartitioner">1.5RDD分区函数(partitioner)</h3>
<p>partitioner就是RDD分区函数，目前Spark实现了两种类型的分区函数，即HashPartitioner（哈希分区）和RangePartitioner（区域分区），且partitioner这个属性只存在（K,V）类型的RDD中，对于非（K,V）类型的partitioner的值就是None。partitioner函数既决定了RDD本身的分区数量，也可作为其父RDD Shuffle输出（MapOutput）中每个分区进行数据切割的依据。</p>
<h2 id="2">2.创建操作</h2>
<h3 id="21">2.1集合创建操作</h3>
<p>RDD的形成可以由内部集合类型来生成，Spark中提供了parallelize和makeRDD两类函数来实现从集合生成RDD，两个函数接口功能类似，不同的是makeRDD还提供了一个可以指定每一个分区preferredLocations参数的实现版本。</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="nx">sc.makeRDD</span><span class="p">(</span><span class="mi">1</span> <span class="k">to</span> <span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="nx">rdd</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">0</span><span class="cp">]</span> at makeRDD at <span class="nt">&lt;console&gt;</span>:27

scala&gt; val collect = Seq((1 to 10,Seq(&quot;host1&quot;,&quot;host3&quot;)),(11 to 20,Seq(&quot;host2&quot;)))
collect: Seq<span class="cp">[</span><span class="p">(</span><span class="nx">scala.collection.immutable.Range.Inclusive</span><span class="p">,</span> <span class="nx">Seq</span><span class="err">[</span><span class="kt">String</span><span class="cp">]</span>)] = List((Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),List(host1, host3)), (Range(11, 12, 13, 14, 15, 16, 17, 18, 19, 20),List(host2)))

scala&gt; val rdd = sc.makeRDD(collect)
rdd: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="nx">scala.collection.immutable.Range.Inclusive</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">1</span><span class="cp">]</span> at makeRDD at <span class="nt">&lt;console&gt;</span>:29

scala&gt; rdd.preferredLocations(rdd.partitions(0))
res1: Seq<span class="cp">[</span><span class="kt">String</span><span class="cp">]</span> = List(host1, host3)

scala&gt; rdd.preferredLocations(rdd.partitions(1))
res2: Seq<span class="cp">[</span><span class="kt">String</span><span class="cp">]</span> = List(host2)
</pre></div>


<h3 id="22">2.2存储创建操作</h3>
<p>Spark的整个生态系统与Hadoop是完全兼容的，对于Hadoop所支持的文件类型或者数据库类型，Spark也同样支持。hadoopRDD和newhadoopRDD是最为抽象的两个函数，主要包括以下四个参数：</p>
<p>输入格式：指定数据输入的类型，如TextInputFormat;</p>
<p>键类型：指定[K,V]键值对中K的类型;</p>
<p>值类型：指定[K,V]键值对中V的类型;</p>
<p>分区值：指定由外部存储生成的RDD的partition数量的最小值，如果没有指定，系统会使用默认值defaultMinSplits;</p>
<p>兼容旧版本Hadoop API的创建操作，</p>
<table>
<thead>
<tr>
<th></th>
<th>文件路径</th>
<th>输入格式</th>
<th>键类型</th>
<th>值类型</th>
<th>分区值</th>
</tr>
</thead>
<tbody>
<tr>
<td>textFile</td>
<td>path</td>
<td>TextInputFormat</td>
<td>LongWritable</td>
<td>Text</td>
<td>minSplits</td>
</tr>
<tr>
<td>hadoopFile</td>
<td>path</td>
<td>F</td>
<td>K</td>
<td>V</td>
<td>minSplits</td>
</tr>
<tr>
<td>hadoopFile</td>
<td>path</td>
<td>F</td>
<td>K</td>
<td>V</td>
<td>DefaultMinSplits</td>
</tr>
<tr>
<td>sequenceFile</td>
<td>path</td>
<td>SequenceFileInputFormat</td>
<td>K</td>
<td>V</td>
<td>minSplits</td>
</tr>
<tr>
<td>sequenceFile</td>
<td>path</td>
<td>SequenceFileInputFormat</td>
<td>K</td>
<td>V</td>
<td>DefaultMinSplits</td>
</tr>
<tr>
<td>objectFile</td>
<td>path</td>
<td>SequenceFileInputFormat</td>
<td>NullWritable</td>
<td>BytesWritable</td>
<td>mminSplits</td>
</tr>
<tr>
<td>hadoopRDD</td>
<td>n/a</td>
<td>inpurformatClass</td>
<td>keyClass</td>
<td>valueClass</td>
<td>minSplits</td>
</tr>
</tbody>
</table>
<p>兼容新版本Hadoop API的创建操作，</p>
<table>
<thead>
<tr>
<th></th>
<th>文件路径</th>
<th>输入格式</th>
<th>键类型</th>
<th>值类型</th>
<th>分区值</th>
</tr>
</thead>
<tbody>
<tr>
<td>newAPIHadoopFile</td>
<td>path</td>
<td>F</td>
<td>K</td>
<td>V</td>
<td>n/a</td>
</tr>
<tr>
<td>newAPIHadoopFile</td>
<td>path</td>
<td>F</td>
<td>K</td>
<td>V</td>
<td>n/a</td>
</tr>
<tr>
<td>newAPIHadoopRDD</td>
<td>path</td>
<td>F</td>
<td>K</td>
<td>V</td>
<td>n/a</td>
</tr>
</tbody>
</table>
<h2 id="3">3.转换操作</h2>
<h3 id="31rdd">3.1RDD基本转换操作</h3>
<p>map：将RDD中类型为T的元素，一对一映射为类型为U的元素。</p>
<p>distinct：返回RDD中所有不一样的元素。</p>
<p>flatMap：将RDD中的每一个元素进行一对多转换。</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="nx">sc.makeRDD</span><span class="p">(</span><span class="mi">1</span> <span class="k">to</span> <span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="nx">rdd</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">2</span><span class="cp">]</span> at makeRDD at <span class="nt">&lt;console&gt;</span>:27

scala&gt; val mapRDD = rdd.map(x =&gt; x.toFloat)
mapRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="nx">Float</span><span class="cp">]</span> = MapPartitionsRDD<span class="cp">[</span><span class="mi">3</span><span class="cp">]</span> at map at <span class="nt">&lt;console&gt;</span>:29

scala&gt; mapRDD.collect()
res3: Array<span class="cp">[</span><span class="nx">Float</span><span class="cp">]</span> = Array(1.0, 2.0, 3.0, 4.0, 5.0)

scala&gt; val flatMapRDD = rdd.flatMap(x =&gt; (1 to x))
flatMapRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="nx">Int</span><span class="cp">]</span> = MapPartitionsRDD<span class="cp">[</span><span class="mi">4</span><span class="cp">]</span> at flatMap at <span class="nt">&lt;console&gt;</span>:29

scala&gt; flatMapRDD.collect()
res4: Array<span class="cp">[</span><span class="nx">Int</span><span class="cp">]</span> = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)

scala&gt; val distinctRDD = flatMapRDD.distinct()
distinctRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="nx">Int</span><span class="cp">]</span> = MapPartitionsRDD<span class="cp">[</span><span class="mi">7</span><span class="cp">]</span> at distinct at <span class="nt">&lt;console&gt;</span>:31

scala&gt; distinctRDD.collect()
res6: Array<span class="cp">[</span><span class="nx">Int</span><span class="cp">]</span> = Array(4, 1, 3, 5, 2)
</pre></div>


<p>repartition：repartition只是coalesce接口中shuffle为true的简易实现。</p>
<p>coalesce：主要讨论如何设置shuffle参数，这里分三种情况（假设RDD有N个分区，需要重新划分成M个分区）</p>
<ol>
<li>
<p>如果N &lt; M，一般情况下，N个分区有数据分布不均的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle参数设置为true；</p>
</li>
<li>
<p>如果N &gt; M且N和M差不多(比如说N是1000,M是100)，那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并成M个分区，这时可以将shuffle参数设置为false(在shuffle为false的情况下，设置M &gt; N，coalesce是不起作用的)，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系；</p>
</li>
<li>
<p>如果N &gt; M且N和M差距悬殊(比如说N是1000,M是1)，这个时候如果把shuffle参数设置为false，由于父子RDD是窄依赖，它们同处在一个Stage中，就可能会造成Spark程序运行的并行度不够，从而影响性能。比如在M为1时，由于只有1个分区，所以只会有一个任务在运行，为了使coalesce之前的操作有更好的并行度，可以将shuffle参数设置为true；</p>
</li>
</ol>
<p>randomSplit:根据weights权重将一个RDD切分成多个RDD；</p>
<p>glom：将RDD中每一个分区中类型为T的元素转换成数组Array[T]，这样每一个分区就只有一个数组元素；</p>
<p>union：将两个RDD集合中的数据进行合并，返回两个RDD的并集（包含两个RDD中相同的元素，不会去重）；</p>
<p>intersection：返回两个RDD集合的交集，且交集中不会包含相同的元素；</p>
<p>subtract：如果subtract针对的是A和B两个集合，即操作是val result = A.subtract(B)，那么result中将会包含A中出现且不在B中出现的元素；</p>
<p>intersection和subtract一般情况下都会有shuffle的过程；</p>
<p>mapPartitions：与map转换操作类似，只不过映射函数的输入参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器；</p>
<p>mapPartitionsWithIndex：与mapPartitions功能类似，只是输入参数多了一个分区的ID；</p>
<p>zip：将两个RDD组合成Key/Value（键/值）形式的RDD，默认两个RDD的partition数量以及元素数量都相同，否则相同系统将会抛出异常。</p>
<p>zipPartitinos：将多个RDD按照partition组合成为新的RDD，zipPartitinos需要相互组合的RDD具有相同的分区数，但是对于每个分区中的元素没有要求。</p>
<p>zipWithIndex：是将RDD中的元素和这个元素的ID组合成键/值对，需要启动一个Spark作业来计算每一个分区的开始索引号，以便能顺序索引。</p>
<p>zipWithUniqueId：是将RDD中的元素和一个唯一ID组合成键/值对，不需要这样一个额外的作业。</p>
<h3 id="32rdd">3.2键值RDD转换操作</h3>
<p>partitionBy:与基本转换操作中的repartition功能类似，根据partitioner函数生成新的ShuffledRDD，将原RDD重新分区(其实在repartition中也是先将RDD[T]转化成RDD[K,V]，这里的V是null，然后使用RDD[K,V]作为参数生成ShuffledRDD)。</p>
<p>mapValues:针对[K,V]中的值进行map操作。</p>
<p>flatMapValues:针对[K,V]中的值进行flatMap操作。</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="nx">sc.parallelize</span><span class="p">(</span><span class="kt">Array</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="mi">1</span><span class="p">)</span>
<span class="nx">rdd</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">0</span><span class="cp">]</span> at parallelize at <span class="nt">&lt;console&gt;</span>:27

scala&gt; val partitionByRDD = rdd.partitionBy(new org.apache.spark.HashPartitioner(2))
partitionByRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = ShuffledRDD<span class="cp">[</span><span class="mi">1</span><span class="cp">]</span> at partitionBy at <span class="nt">&lt;console&gt;</span>:29

scala&gt; partitionByRDD.collect()
res0: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = Array((2,1), (1,1), (1,2), (3,1))

scala&gt; val mapValuesRDD = rdd.mapValues(x =&gt; x+1)
mapValuesRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = MapPartitionsRDD<span class="cp">[</span><span class="mi">2</span><span class="cp">]</span> at mapValues at <span class="nt">&lt;console&gt;</span>:29

scala&gt; mapValuesRDD.collect()
res4: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = Array((1,2), (1,3), (2,2), (3,2))

scala&gt; val flatMapValuesRDD = rdd.flatMapValues(x =&gt; Seq(x,&quot;a&quot;))
flatMapValuesRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nb">Any</span><span class="p">)</span><span class="cp">]</span> = MapPartitionsRDD<span class="cp">[</span><span class="mi">3</span><span class="cp">]</span> at flatMapValues at <span class="nt">&lt;console&gt;</span>:29

scala&gt; flatMapValuesRDD.collect()
res5: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nb">Any</span><span class="p">)</span><span class="cp">]</span> = Array((1,1), (1,a), (1,2), (1,a), (2,1), (2,a), (3,1), (3,a))
</pre></div>


<p>combineByKey，foldByKey，reduceByKey，groupByKey：四种键值对转换操作都是针对RDD[K,V]本身，不涉及与其它RDD的组合操作，四种操作类型最终都会归结为对combineByKey的调用。combineByKey接口是将RDD[K,V]转化成返回类型RDD[K,C]，这里V类型与C类型可以相同也可以不相同，combineByKey抽象接口一般需要需要传入以下5个典型参数：</p>
<p>createCombiner:创建组合器函数，将V类型值转换成C类型值；</p>
<p>mergeValue:合并值函数，将一个V类型值和一个C类型值合并成一个C类型值；</p>
<p>mergeCombiners:合并组合器函数，将两个C类型值合并成一个C类型值；</p>
<p>partitioner:指定分区函数；</p>
<p>mapSideCombine:布尔类型值，指定是否需要在Map端进行combine操作，类似于MapReduce中进行的combine操作；</p>
<p>combineByKey内部实现是通过三步来实现，1)根据是否需要在Map端进行combine操作决定是否对RDD先进行一次mapPartitions操作(利用createCombiner，mergeValue，mergeCombiners三个函数)来达到减少shuffle数据量的操作；2)根据partitioner函数对MapPartitionsRDD进行shuffle操作；3)对于shuffle的结果再进行一次combine操作；</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="k">import</span> <span class="nx">scala.collection.mutable.HashSet</span>
<span class="k">import</span> <span class="nx">scala.collection.mutable.HashSet</span>

<span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">bufs</span> <span class="o">=</span> <span class="nx">pairs.mapValues</span><span class="p">(</span><span class="n">v</span> <span class="o">=&gt;</span> <span class="nx">HashSet</span><span class="p">(</span><span class="nx">v</span><span class="p">))</span>
<span class="nx">bufs</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">scala.collection.mutable.HashSet</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>)] = MapPartitionsRDD<span class="cp">[</span><span class="mi">5</span><span class="cp">]</span> at mapValues at <span class="nt">&lt;console&gt;</span>:30

scala&gt; import scala.collection.mutable.HashSet
import scala.collection.mutable.HashSet

scala&gt; val pairs = sc.parallelize(Array((1,1),(1,2),(1,3),(1,1),(2,1)),2)
pairs: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">6</span><span class="cp">]</span> at parallelize at <span class="nt">&lt;console&gt;</span>:30

scala&gt; val bufs = pairs.mapValues(v =&gt; HashSet(v))
bufs: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">scala.collection.mutable.HashSet</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>)] = MapPartitionsRDD<span class="cp">[</span><span class="mi">7</span><span class="cp">]</span> at mapValues at <span class="nt">&lt;console&gt;</span>:32

scala&gt; val sums = bufs.foldByKey(new HashSet<span class="cp">[</span><span class="nx">Int</span><span class="cp">]</span>)(_ ++= _)
sums: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">scala.collection.mutable.HashSet</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>)] = ShuffledRDD<span class="cp">[</span><span class="mi">8</span><span class="cp">]</span> at foldByKey at <span class="nt">&lt;console&gt;</span>:34

scala&gt; sums.collect()
res7: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">scala.collection.mutable.HashSet</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>)] = Array((2,Set(1)), (1,Set(1, 2, 3)))

scala&gt; val reduceByKeyRDD = pairs.reduceByKey(_+_)
reduceByKeyRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = ShuffledRDD<span class="cp">[</span><span class="mi">9</span><span class="cp">]</span> at reduceByKey at <span class="nt">&lt;console&gt;</span>:32

scala&gt; reduceByKeyRDD.collect()
res8: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = Array((2,1), (1,7))

scala&gt; val groupByKeyRDD = pairs.groupByKey()
groupByKeyRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Iterable</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>)] = ShuffledRDD<span class="cp">[</span><span class="mi">10</span><span class="cp">]</span> at groupByKey at <span class="nt">&lt;console&gt;</span>:32

scala&gt; groupByKeyRDD.collect()
res9: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Iterable</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>)] = Array((2,CompactBuffer(1)), (1,CompactBuffer(1, 2, 3, 1)))
</pre></div>


<p>join、leftOuterJoin、rightOuterJoin都是针对RDD[K,V]中K值相等的连接操作，分别对应内连接、左外连接、右外连接，最终都会调用cogroup来实现。而subtractByKey和基本转换操作subtract类似，只是针对RDD[K,V]中的K值来进行操作。</p>
<div class="hlcode"><pre><span class="nx">scala</span><span class="o">&gt;</span> <span class="nx">val</span> <span class="n">rdd1</span> <span class="o">=</span> <span class="nx">sc.parallelize</span><span class="p">(</span><span class="kt">Array</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="mi">1</span><span class="p">)</span>
<span class="nx">rdd1</span><span class="p">:</span> <span class="nx">org.apache.spark.rdd.RDD</span><span class="err">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">18</span><span class="cp">]</span> at parallelize at <span class="nt">&lt;console&gt;</span>:30

scala&gt; val rdd2 = sc.parallelize(Array((1,&#39;x&#39;),(2,&#39;y&#39;),(2,&#39;z&#39;),(4,&#39;w&#39;)),1)
rdd2: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Char</span><span class="p">)</span><span class="cp">]</span> = ParallelCollectionRDD<span class="cp">[</span><span class="mi">19</span><span class="cp">]</span> at parallelize at <span class="nt">&lt;console&gt;</span>:30

scala&gt; val cogroupRDD = rdd1.cogroup(rdd2)
cogroupRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="p">(</span><span class="nx">Iterable</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>, Iterable<span class="cp">[</span><span class="nx">Char</span><span class="cp">]</span>))] = MapPartitionsRDD<span class="cp">[</span><span class="mi">21</span><span class="cp">]</span> at cogroup at <span class="nt">&lt;console&gt;</span>:34

scala&gt; cogroupRDD.collect()
res13: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="p">(</span><span class="nx">Iterable</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>, Iterable<span class="cp">[</span><span class="nx">Char</span><span class="cp">]</span>))] = Array((4,(CompactBuffer(),CompactBuffer(w))), (1,(CompactBuffer(1, 2),CompactBuffer(x))), (3,(CompactBuffer(1),CompactBuffer())), (2,(CompactBuffer(1),CompactBuffer(y, z))))

scala&gt; val joinRDD = rdd1.join(rdd2)
joinRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Char</span><span class="p">))</span><span class="cp">]</span> = MapPartitionsRDD<span class="cp">[</span><span class="mi">24</span><span class="cp">]</span> at join at <span class="nt">&lt;console&gt;</span>:34

scala&gt; joinRDD.collect()
res14: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Char</span><span class="p">))</span><span class="cp">]</span> = Array((1,(1,x)), (1,(2,x)), (2,(1,y)), (2,(1,z)))

scala&gt; val leftOuterJoinRDD = rdd1.leftOuterJoin(rdd2)
leftOuterJoinRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nb">Option</span><span class="err">[</span><span class="nx">Char</span><span class="cp">]</span>))] = MapPartitionsRDD<span class="cp">[</span><span class="mi">27</span><span class="cp">]</span> at leftOuterJoin at <span class="nt">&lt;console&gt;</span>:34

scala&gt; leftOuterJoinRDD.collect()
res15: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nb">Option</span><span class="err">[</span><span class="nx">Char</span><span class="cp">]</span>))] = Array((1,(1,Some(x))), (1,(2,Some(x))), (3,(1,None)), (2,(1,Some(y))), (2,(1,Some(z))))

scala&gt; val rightOuterJoinRDD = rdd1.rightOuterJoin(rdd2)
rightOuterJoinRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="p">(</span><span class="nb">Option</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>, Char))] = MapPartitionsRDD<span class="cp">[</span><span class="mi">30</span><span class="cp">]</span> at rightOuterJoin at <span class="nt">&lt;console&gt;</span>:34

scala&gt; rightOuterJoinRDD.collect()
res16: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="p">(</span><span class="nb">Option</span><span class="err">[</span><span class="nx">Int</span><span class="cp">]</span>, Char))] = Array((4,(None,w)), (1,(Some(1),x)), (1,(Some(2),x)), (2,(Some(1),y)), (2,(Some(1),z)))

scala&gt; val subtractByKeyRDD = rdd1.subtractByKey(rdd2)
subtractByKeyRDD: org.apache.spark.rdd.RDD<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = SubtractedRDD<span class="cp">[</span><span class="mi">31</span><span class="cp">]</span> at subtractByKey at <span class="nt">&lt;console&gt;</span>:34

scala&gt; subtractByKeyRDD.collect()
res17: Array<span class="cp">[</span><span class="p">(</span><span class="nx">Int</span><span class="p">,</span> <span class="nx">Int</span><span class="p">)</span><span class="cp">]</span> = Array((3,1))
</pre></div>


<h3 id="33rdd">3.3RDD依赖关系</h3>
<p>转换操作构建了RDD之间的大部分依赖关系，但是Spark内部生成的RDD对象数量一般多于用户书写的Spark应用程序包含的RDD，根本原因就是Spark的一些操作与RDD不是一一对应的。</p>
<h2 id="4control-operation">4.控制操作(control operation)</h2>
<h2 id="5action-operation">5.行动操作(action operation)</h2>
<h3 id="51">5.1集合标量行动操作</h3>
<h3 id="52">5.2存储行动操作</h3>
</div>

        </div>
        <div id="footer">
            <span>
                Copyright © 2016 zhbzz2007.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        
    </body>
</html>